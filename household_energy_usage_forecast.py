# -*- coding: utf-8 -*-
"""Household Energy Usage Forecast

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FrbqvPXmFuc5SqlFkhUYvqUcAhCdts4n

New code
***Step 1:Import Libraries and Load the given Dataset and Explore the Dataset***
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

df = pd.read_csv('/content/sample_data/household_power_consumption.txt', sep=';', low_memory=False, na_values=['nan','?'])
print(df.columns)

"""*Convert datetime Column with dayfirst=True:*"""

if 'Date' in df.columns and 'Time' in df.columns:
    df['datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], dayfirst=True, errors='coerce')
    df.drop(['Date', 'Time'], axis=1, inplace=True)
else:
    print("Date and/or Time columns are missing.")

"""*Explore the Dataset:*"""

df.head()
df.info()
df.describe()

"""**Step 2: Data Preprocessing**
*Handle Missing Values:*
"""

df.fillna(df.mean(), inplace=True)

"""*Parse Date and Time into Separate Features*"""

df['year'] = df['datetime'].dt.year
df['month'] = df['datetime'].dt.month
df['day'] = df['datetime'].dt.day
df['hour'] = df['datetime'].dt.hour

"""*Create Additional Features:*"""

df['daily_avg'] = df['Global_active_power'].rolling(window=24).mean()
print(df.columns)

"""*Normalize/Scale Data:*"""

scaler = StandardScaler()
df[['Global_active_power']] = scaler.fit_transform(df[['Global_active_power']])
df.head()

# Time series plot
plt.figure(figsize=(10, 6))
sns.lineplot(data=df, x='datetime', y='Global_active_power')
plt.title('Global Active Power over Time')
plt.xlabel('Datetime')
plt.ylabel('Global Active Power (kilowatts)')
plt.show()

# Compute the correlation matrix
corr_matrix = df.corr()

# Plot the heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()

# Pair plot to visualize relationships
sns.pairplot(df[['Global_active_power', 'Global_reactive_power', 'Voltage', 'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']])
plt.show()

# Box plot to identify outliers
plt.figure(figsize=(10, 6))
sns.boxplot(data=df[['Global_active_power', 'Global_reactive_power', 'Voltage', 'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']])
plt.title('Box Plot for Outliers')
plt.show()

"""***Step 3 : Model Selection and Training ***

*Ensure DataFrame is Defined and Preprocessed:*
"""

from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Define features and target variable
features = ['year', 'month', 'day', 'hour', 'daily_avg']
X = df[features]
y = df['Global_active_power']

# Impute missing values
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42)

"""*Train Regression Models:*
*Linear Regression:*
"""

lr = LinearRegression()
lr.fit(X_train, y_train)

"""*Random Forest*
Random Forest model is consuming too much memory. Here are a few strategies to handle this issue:

Use a Smaller Subset of Data
"""

df_sample = df.sample(frac=0.1, random_state=42)

"""Reduce the Number of Trees"""

rf = RandomForestRegressor(n_estimators=50)  # Default is 100
rf.fit(X_train, y_train)

"""Use a Smaller Max Depth: Limit the depth of the trees to reduce memory usage:"""

rf = RandomForestRegressor(max_depth=10)  # Adjust based on your needs
rf.fit(X_train, y_train)

"""**Use Dask for Parallel Computing**
Dask can handle larger-than-memory computations by parallelizing operations
"""

!pip install dask[dataframe]
import dask.dataframe as dd

ddf = dd.from_pandas(df, npartitions=10)
df = ddf.compute()

"""*HistGradientBoosting*"""

from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingRegressor

hgb = HistGradientBoostingRegressor()
hgb.fit(X_train, y_train)

"""*Gradient Boosting:*"""

gb = GradientBoostingRegressor()
gb.fit(X_train, y_train)

"""*Evaluate Models:*"""

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

models = {'Linear Regression': lr, 'Random Forest': rf, 'Gradient Boosting': gb, 'HistGradientBoosting': hgb}
for name, model in models.items():
    y_pred = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))  # Calculate RMSE manually
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"{name} - RMSE: {rmse}, MAE: {mae}, R2: {r2}")

"""**Best-Performing Model**
Based on the evaluation metrics, the best-performing model is ***HistGradientBoosting*** with the following scores:

RMSE: 0.4933
MAE: 0.2674
R²: 0.7567

How the Best-Performing Model is Identified
The best-performing model is identified by comparing the evaluation metrics:

Root Mean Squared Error (RMSE): Measures prediction accuracy. Lower RMSE indicates better accuracy.
Mean Absolute Error (MAE): Evaluates average error magnitude. Lower MAE indicates better performance.
R-Squared (R²): Indicates how well the model explains the variability of the target variable. Higher R² indicates better explanatory power.
The HistGradientBoosting model has the lowest RMSE and MAE, and the highest R², making it the best-performing model among the ones evaluated.

**Visualization of Energy Trends and Predictive Performance**
Effective visualizations can help in understanding energy trends and the performance of the predictive model. Here are some suggested visualizations:

**Time Series Plot:** Visualize Global_active_power over time to identify trends and patterns.
"""

plt.figure(figsize=(10, 6))
sns.lineplot(data=df, x='datetime', y='Global_active_power')
plt.title('Global Active Power over Time')
plt.show()

"""**Model Predictions vs Actual**: Compare the model's predictions with actual values.

These visualizations will help you present your findings effectively and provide actionable insights into energy usage trends
"""

plt.figure(figsize=(10, 6))
plt.plot(y_test.values, label='Actual')
plt.plot(hgb.predict(X_test), label='HistGradientBoosting')
plt.legend()
plt.title('Model Predictions vs Actual')
plt.show()

"""**Let's summarize the key outcomes of your project:**
1. Accurate Prediction Model for Household Power Consumption
The *HistGradientBoosting* model was identified as the best-performing model with the following metrics:

RMSE: 0.4934
MAE: 0.2673
R²: 0.7567 **bold text**
This model provides accurate predictions of household power consumption, which can be used for better energy management and planning.

2. Clear Insights into Key Factors Influencing Energy Usage
Feature importance analysis helps in understanding which factors most influence energy usage. For example, features like hour, day, month, and daily_avg can provide insights into usage patterns:

Hour: Identifies peak usage times during the day.
Day: Highlights daily consumption trends.
Month: Shows seasonal variations in energy usage.
Daily Average: Provides a smoothed view of consumption over time.
"""

!pip install xgboost

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Assuming df is already loaded and preprocessed
features = ['year', 'month', 'day', 'hour', 'daily_avg']
X = df[features]
y = df['Global_active_power']

# Impute missing values
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42)

xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
xgb_model.fit(X_train, y_train)

y_pred = xgb_model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"XGBoost - RMSE: {rmse}, MAE: {mae}, R2: {r2}")

models = {
    'Linear Regression': lr,
    'Random Forest': rf,
    'Gradient Boosting': gb,
    'HistGradientBoosting': hgb,
    'XGBoost': xgb_model
}

for name, model in models.items():
    y_pred = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))  # Calculate RMSE manually
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"{name} - RMSE: {rmse}, MAE: {mae}, R2: {r2}")

import matplotlib.pyplot as plt
import seaborn as sns
model_names = ['Linear Regression', 'Random Forest', 'Gradient Boosting', 'HistGradientBoosting', 'XGBoost']
rmse_values = [0.5025, 0.4946, 0.4964, 0.4934, 0.4947]
mae_values = [0.2705, 0.2673, 0.2695, 0.2674, 0.2681]
r2_values = [0.7476, 0.7555, 0.7537, 0.7567, 0.7554]

plt.figure(figsize=(10, 6))
sns.barplot(x=model_names, y=rmse_values, hue=model_names, palette='viridis', dodge=False, legend=False)
plt.title('RMSE Comparison')
plt.xlabel('Model')
plt.ylabel('RMSE')
plt.show()

plt.figure(figsize=(10, 6))
sns.barplot(x=model_names, y=mae_values, hue=model_names, palette='viridis', dodge=False, legend=False)
plt.title('MAE Comparison')
plt.xlabel('Model')
plt.ylabel('MAE')
plt.show()

plt.figure(figsize=(10, 6))
sns.barplot(x=model_names, y=r2_values, hue=model_names, palette='viridis', dodge=False, legend=False)
plt.title('R² Comparison')
plt.xlabel('Model')
plt.ylabel('R²')
plt.show()

!pip freeze > requirements.txt

"""Hyperparamter Turning"""

from sklearn.model_selection import GridSearchCV

rf_param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [None, 10],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

gb_param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

hgb_param_grid = {
    'max_iter': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [None, 10, 20],
    'min_samples_leaf': [20, 50, 100]
}

from sklearn.model_selection import RandomizedSearchCV

rf_random_search = RandomizedSearchCV(estimator=RandomForestRegressor(), param_distributions=rf_param_grid, n_iter=10, cv=3, n_jobs=-1, scoring='neg_mean_squared_error', random_state=42)
rf_random_search.fit(X_train, y_train)
print(f"Best parameters for Random Forest: {rf_random_search.best_params_}")